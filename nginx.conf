# Optimized nginx configuration for high-throughput k6 load testing
# Supports 350-500 VUs with 2k+ requests/sec

# Use all available CPU cores for better throughput
worker_processes auto;

events {
    # Increased worker connections for high concurrency (350-500 VUs)
    # Each VU can have multiple connections, so we need headroom
    worker_connections 4096;
    
    # Use epoll for better performance on Linux
    use epoll;
    
    # Allow multiple connections per worker
    multi_accept on;
}

http {
    
    upstream app_backend {
        # Round-robin load balancing across 2 app instances
        server app1:8081;
        server app2:8082;
        
        # Enable keepalive connections to backend (reduces connection overhead)
        keepalive 64;
        keepalive_requests 100;
        keepalive_timeout 60s;
    }

    # Logging (can be disabled for maximum performance)
    access_log /var/log/nginx/access.log;
    error_log /var/log/nginx/error.log;

    # Basic settings optimized for high throughput
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    
    # Keepalive settings for client connections
    keepalive_timeout 65;
    keepalive_requests 1000;
    
    # Hash table sizes (prevents hash bucket memory errors under load)
    types_hash_max_size 2048;
    server_names_hash_bucket_size 128;
    client_max_body_size 1m;  # For JSON payloads

    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Gzip compression (disabled for load testing to reduce CPU overhead)
    # Enable if you need compression, but it adds latency and CPU usage
    # gzip on;
    # gzip_vary on;
    # gzip_proxied any;
    # gzip_comp_level 6;
    # gzip_types text/plain text/css text/xml text/javascript application/json application/javascript application/xml+rss;

    server {
        listen 8080;
        server_name _;

        # Timeouts optimized for load testing
        proxy_connect_timeout 10s;      # Faster connection timeout
        proxy_send_timeout 10s;          # Faster send timeout
        proxy_read_timeout 10s;         # Faster read timeout
        
        # Enable HTTP/1.1 for keepalive
        proxy_http_version 1.1;
        proxy_set_header Connection "";  # Clear connection header for keepalive

        # Buffer settings optimized for high throughput
        proxy_buffering on;
        proxy_buffer_size 8k;            # Increased from 4k
        proxy_buffers 16 8k;             # Increased: 16 buffers of 8k = 128k total
        proxy_busy_buffers_size 16k;     # Increased from 8k
        proxy_temp_file_write_size 16k;  # Temp file write size
        
        # Disable buffering for streaming responses (if needed)
        # proxy_buffering off;

        # Headers
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Health check endpoint (optimized)
        location /actuator/health {
            proxy_pass http://app_backend;
            access_log off;              # Disable logging for health checks
            proxy_connect_timeout 2s;    # Faster timeout for health checks
            proxy_read_timeout 2s;
        }

        # API endpoints (main traffic)
        location /api/ {
            proxy_pass http://app_backend;
            
            # Optimizations for API endpoints
            proxy_connect_timeout 10s;
            proxy_send_timeout 10s;
            proxy_read_timeout 10s;
        }

        # All other requests
        location / {
            proxy_pass http://app_backend;
        }
    }
}

